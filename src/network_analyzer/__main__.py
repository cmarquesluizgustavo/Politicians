import os
import time
import pickle
import threading
import pandas as pd
from base_logger import NetworkAnalyzerLogger
from basic_statistics import BasicStatistics
from similarity_statistics import SimilarityAndGainsStatistics

MAX_THREADS = 8

logger = NetworkAnalyzerLogger(
    name="AllNetworks",
    client_class="ThreadRunner",
    log_level=20,
    log_file="logs/network_analyzer/thread_runner/thread_runner.log",
    terminal=True,
)


def get_statistics_4_network(
    file: str,
    target_features: list,
    similarity_algorithms: list,
    save_path: str,
    semaphore: threading.Semaphore,
):
    """
    Get and save all statistics for the network
    """
    try:
        g = pickle.load(open(file, "rb"))
        bs = BasicStatistics(g)

        bs.network_to_dataframe().to_csv(
            f"{save_path}/networks/{g.name}_network.csv", index=False
        )
        bs.nodes_to_dataframe().to_csv(
            f"{save_path}/nodes/{g.name}_nodes.csv", index=True
        )

        for similarity_algorithm in similarity_algorithms:
            get_statistics_4_similarity_algorithm(
                g, target_features, similarity_algorithm, save_path
            )

    except Exception as e:
        logger.error("Error in file %s: %s", file, e)

    semaphore.release()


def get_statistics_4_similarity_algorithm(
    g,
    target_features,
    similarity_algorithm,
    save_path,
):
    """
    Get and save all statistics for the network using a specific similarity algorithm
    """
    os.makedirs(f"{save_path}/features/{similarity_algorithm}/network", exist_ok=True)
    os.makedirs(f"{save_path}/features/{similarity_algorithm}/nodes", exist_ok=True)

    sgs = SimilarityAndGainsStatistics(
        g,
        target_features,
        similarity_algorithm,
    )
    sgs.gains_by_node.to_csv(
        f"{save_path}/features/{similarity_algorithm}/nodes/{g.name}_nodes.csv"
    )

    for feature, feature_data in sgs.gains_by_feature.items():
        feature_data.to_csv(
            f"{save_path}/features/{similarity_algorithm}/network/{g.name}_{feature}.csv",
            index=False,
        )


def consolidate_files_4_algorithm(
    path: str, similarity_algorithm: str, target_features: list
):
    network_features_files = os.listdir(
        f"{path}/features/{similarity_algorithm}/network"
    )

    node_features_files = os.listdir(f"{path}/features/{similarity_algorithm}/nodes")

    nodes_df = pd.concat(
        [
            pd.read_csv(f"{path}/features/{similarity_algorithm}/nodes/{node_file}")
            for node_file in node_features_files
        ],
    )

    nodes_df = nodes_df.sort_values(by=["period"]).set_index("period")
    nodes_df.to_csv(f"{path}/features/{similarity_algorithm}/nodes.csv")

    for feature in target_features:
        feature_periods = pd.concat(
            [
                pd.read_csv(
                    f"{path}/features/{similarity_algorithm}/network/{network_feature_file}"
                )
                for network_feature_file in network_features_files
                if feature in network_feature_file
            ],
        )
        # Remove columns with all NaNs
        feature_periods = feature_periods.drop(columns=["Unnamed: 0"], errors="ignore")
        feature_periods = feature_periods.loc[:, feature_periods.notna().any()]
        feature_periods.to_csv(f"{path}/features/{similarity_algorithm}/{feature}.csv")


def consolidate_files(path: str, similarity_algorithms: list, target_features: list):
    """
    This function gathers all the csv files generated by the runs
    and consolidates them into a few csv files that contain the data.
    """

    networks_files = os.listdir(f"{path}/networks")
    nodes_files = os.listdir(f"{path}/nodes")
    networks_df = pd.concat(
        [
            pd.read_csv(f"{path}/networks/{network_file}")
            for network_file in networks_files
        ],
    )
    nodes_df = pd.concat(
        [pd.read_csv(f"{path}/nodes/{node_file}") for node_file in nodes_files],
    )

    nodes_df = nodes_df[nodes_df["neighbors"] != 0]

    networks_df = networks_df.sort_values(by=["period"])
    nodes_df = nodes_df.sort_values(by=["node_id", "period"])

    networks_df.to_csv(f"{path}/networks/networks.csv", index=False)
    nodes_df.to_csv(f"{path}/nodes/nodes.csv", index=False)

    for similarity_algorithm in similarity_algorithms:
        consolidate_files_4_algorithm(path, similarity_algorithm, target_features)


def main(
    files: list,
    target_features: list,
    similarity_algorithms: list,
    save_path: str,
):
    """
    Runs threads to get statistics for all networks, limiting the number of concurrent threads.
    """

    os.makedirs(save_path, exist_ok=True)
    os.makedirs(f"{save_path}/networks", exist_ok=True)
    os.makedirs(f"{save_path}/nodes", exist_ok=True)

    logger.info("Creating threads to get statistics for all networks.")
    threads = []
    semaphore = threading.Semaphore(MAX_THREADS)
    for file in files:
        semaphore.acquire()
        t = threading.Thread(
            target=get_statistics_4_network,
            args=(
                file,
                target_features.copy(),
                similarity_algorithms.copy(),
                save_path,
                semaphore,
            ),
        )
        threads.append(t)
        time.sleep(1.5)  # Avoids two threads to have the same name and log file
        t.start()

    logger.info("Waiting for all threads to finish.")
    for t in threads:
        t.join()

    logger.info("All threads finished.")
    consolidate_files(save_path, similarity_algorithms, target_features)
    logger.info("Files consolidated.")


if __name__ == "__main__":
    TARGET_FEATURES = [
        "siglaPartido",
        "siglaUf",
        "education",
        "gender",
        "region",
        "occupation",
        "ethnicity",
        "age_group",
    ]
    SIMILARITY_ALGORITHMS = ["adamic_adar", "jaccard"]
    FILES = [
        f"data/network_builder/{file}"
        for file in os.listdir("data/network_builder")
        if file.endswith(".pkl")
    ]
    FILES.sort()
    SAVE_PATH = "data/network_analyzer"
    main(FILES, TARGET_FEATURES, SIMILARITY_ALGORITHMS, SAVE_PATH)
